{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-------------+----------+-------+------+---------+----+-------+------+-------------+-----------------+--------------+--------------+\n",
      "|_c0|     ItinID|        MktID|MktCoupons|Quarter|Origin|OriginWac|Dest|DestWac| Miles|ContiguousUSA|NumTicketsOrdered|AirlineCompany|PricePerTicket|\n",
      "+---+-----------+-------------+----------+-------+------+---------+----+-------+------+-------------+-----------------+--------------+--------------+\n",
      "|  0|20181767585|2018176758501|         1|      1|   PHL|       23| LAX|     91|2402.0|            2|              1.0|            AA|        672.87|\n",
      "|  1|20181767586|2018176758601|         1|      1|   PHL|       23| LAX|     91|2402.0|            2|              1.0|            AA|        367.68|\n",
      "|  2|20181767587|2018176758701|         1|      1|   PHL|       23| LAX|     91|2402.0|            2|              1.0|            AA|        417.94|\n",
      "|  3|20181767636|2018176763601|         1|      1|   PHL|       23| LAX|     91|2402.0|            2|              1.0|            AA|         247.1|\n",
      "|  4|20181767637|2018176763701|         1|      1|   PHL|       23| LAX|     91|2402.0|            2|              1.0|            AA|        276.35|\n",
      "+---+-----------+-------------+----------+-------+------+---------+----+-------+------+-------------+-----------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe = spark.read.option(\"inferschema\", \"true\").option(\"header\", \"true\").csv(\"flight06.csv\")\n",
    "dataframe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#133 ASC NULLS FIRST], output=[Dest#23,destination_total#133])\n",
      "+- *(2) HashAggregate(keys=[Dest#23], functions=[sum(miles#25)])\n",
      "   +- Exchange hashpartitioning(Dest#23, 200), true, [id=#50]\n",
      "      +- *(1) HashAggregate(keys=[Dest#23], functions=[partial_sum(miles#25)])\n",
      "         +- FileScan csv [Dest#23,Miles#25] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/Dawood Rana/Big Data/flight06.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Dest:string,Miles:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.groupBy(\"Dest\").sum(\"miles\").withColumnRenamed(\"sum(miles)\", \"destination_total\").sort(\"destination_total\").limit(5).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe = spark.read.option(\"inferschema\", \"true\").option(\"header\", \"true\").csv(\"summary06.csv\")\n",
    "dataframe.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Sort [ORIGIN_COUNTRY_NAME#155 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(ORIGIN_COUNTRY_NAME#155 ASC NULLS FIRST, 200), true, [id=#118]\n",
      "   +- *(2) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#155], functions=[sum(cast(count#156 as bigint))])\n",
      "      +- Exchange hashpartitioning(ORIGIN_COUNTRY_NAME#155, 200), true, [id=#114]\n",
      "         +- *(1) HashAggregate(keys=[ORIGIN_COUNTRY_NAME#155], functions=[partial_sum(cast(count#156 as bigint))])\n",
      "            +- *(1) Project [ORIGIN_COUNTRY_NAME#155, count#156]\n",
      "               +- *(1) Filter (isnotnull(DEST_COUNTRY_NAME#154) AND (DEST_COUNTRY_NAME#154 = United States))\n",
      "                  +- FileScan csv [DEST_COUNTRY_NAME#154,ORIGIN_COUNTRY_NAME#155,count#156] Batched: false, DataFilters: [isnotnull(DEST_COUNTRY_NAME#154), (DEST_COUNTRY_NAME#154 = United States)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/Dawood Rana/Big Data/summary06.csv], PartitionFilters: [], PushedFilters: [IsNotNull(DEST_COUNTRY_NAME), EqualTo(DEST_COUNTRY_NAME,United States)], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 01\n",
    "from pyspark.sql.functions import col, sum\n",
    "dataframe.where(col(\"DEST_COUNTRY_NAME\")==\"United States\").groupBy(\"ORIGIN_COUNTRY_NAME\").agg(sum(\"count\")).sort(\"ORIGIN_COUNTRY_NAME\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|Dest|  sum(miles)|\n",
      "+----+------------+\n",
      "| PSE|   3157476.0|\n",
      "| DLG|     26978.0|\n",
      "| MSY|1.21232583E8|\n",
      "| PPG|    862868.0|\n",
      "| GEG| 1.5338492E7|\n",
      "| SNA| 7.3301545E7|\n",
      "| BUR| 2.4765359E7|\n",
      "| GRB|    227066.0|\n",
      "| GTF|   1253347.0|\n",
      "| IDA|    910594.0|\n",
      "| GRR| 1.4140422E7|\n",
      "| LWB|       489.0|\n",
      "| PVU|    674614.0|\n",
      "| PSG|    226663.0|\n",
      "| EUG|   2720454.0|\n",
      "| PVD| 2.6924425E7|\n",
      "| GSO|   1940195.0|\n",
      "| MYR|   8359385.0|\n",
      "| OAK|1.10610381E8|\n",
      "| MSN|   5571200.0|\n",
      "+----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe = spark.read.option(\"inferschema\", \"true\").option(\"header\", \"true\").csv(\"flight06.csv\")\n",
    "dataframe.groupBy(\"Dest\").agg(sum(\"miles\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Dest|destination_total|\n",
      "+----+-----------------+\n",
      "| ITH|            172.0|\n",
      "| LAN|            179.0|\n",
      "| MBS|            222.0|\n",
      "| DRO|            250.0|\n",
      "| LWB|            489.0|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 02: count total destination of flights\n",
    "dataframe.groupBy(\"Dest\").agg(sum(\"miles\")).withColumnRenamed(\"sum(miles)\",\"destination_total\").orderBy(\"destination_total\").limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
